{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from preprocessing import Preprocess as preprocess_class\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_object=preprocess_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To clean self-closing tags and both closing tags from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/a/11229866\n",
    "self_contained_ref_regex=r\"<[^>]*>\"\n",
    "self_contained_ref_pattern=re.compile(self_contained_ref_regex)\n",
    "\n",
    "def clean_tags_prevent_content(curr_text):\n",
    "    curr_text=self_contained_ref_pattern.sub(\" \",curr_text)\n",
    "    return curr_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_newline_stuff(curr_text):\n",
    "    curr_text=curr_text.replace(\"\\n\",\"\")\n",
    "    curr_text=curr_text.replace(\"\\t\",\"\")\n",
    "    return curr_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_tags_list(tag_text):\n",
    "    arr=tag_text.split(\">\")\n",
    "    arr=list(filter(lambda x:x!=\"\",arr))\n",
    "    arr=[x[1:] for x in arr]\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_regex = re.compile(\"duplicate\", re.IGNORECASE)\n",
    "def rem_dup(text):\n",
    "    return dup_regex.sub(\" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_refs_regex=r\"<code(.*?)<\\/code>\"\n",
    "paired_refs_pattern=re.compile(paired_refs_regex)\n",
    "def rem_code(text):\n",
    "    return paired_refs_pattern.sub(\" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_stuff.ipynb   ids_list.json\t      preprocessing.py\r\n",
      "clean_data.ipynb   make_best_dataset.ipynb    __pycache__\r\n",
      "dups_mapping.json  master_slave_mapping.json  save_all_useful_ids.ipynb\r\n",
      "fill_dupes.ipynb   merge_data.ipynb\t      stopwords.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dups_mapping.json\",'r') as fd:\n",
    "    dups_dict=json.load(fd)\n",
    "    dups_dict=dups_dict['dups_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46419"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dups_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX_FILE_PATH=\"../data_files/pp_result/result\"\n",
    "NEW_PREFIX_FILE_PATH=\"../data_files/super_pp_result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting  0\n",
      "Finished  0\n",
      "dups potential  211\n",
      "-----------\n",
      "Starting  1\n",
      "Finished  1\n",
      "dups potential  488\n",
      "-----------\n",
      "Starting  2\n",
      "Finished  2\n",
      "dups potential  787\n",
      "-----------\n",
      "Starting  3\n",
      "Finished  3\n",
      "dups potential  1114\n",
      "-----------\n",
      "Starting  4\n",
      "Finished  4\n",
      "dups potential  1422\n",
      "-----------\n",
      "Starting  5\n",
      "Finished  5\n",
      "dups potential  1717\n",
      "-----------\n",
      "Starting  6\n",
      "Finished  6\n",
      "dups potential  2033\n",
      "-----------\n",
      "Starting  7\n",
      "Finished  7\n",
      "dups potential  2355\n",
      "-----------\n",
      "Starting  8\n",
      "Finished  8\n",
      "dups potential  2674\n",
      "-----------\n",
      "Starting  9\n",
      "Finished  9\n",
      "dups potential  2949\n",
      "-----------\n",
      "Starting  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/anmolagarwal/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-12-1d37734bfdc8>\", line 21, in <module>\n",
      "    new_val['body_vec']=preprocess_object.parse_string(new_val['cleaned_body'])\n",
      "  File \"/home/anmolagarwal/Desktop/mounted_dump/smai_project_team_24/find_dupes/preprocessing.py\", line 60, in parse_string\n",
      "    tokenized_text = self.stemming(tokenized_text)\n",
      "  File \"/home/anmolagarwal/Desktop/mounted_dump/smai_project_team_24/find_dupes/preprocessing.py\", line 42, in stemming\n",
      "    stemmed_words = [self.ps.stem(word) for word in text]\n",
      "  File \"/home/anmolagarwal/Desktop/mounted_dump/smai_project_team_24/find_dupes/preprocessing.py\", line 42, in <listcomp>\n",
      "    stemmed_words = [self.ps.stem(word) for word in text]\n",
      "  File \"/home/anmolagarwal/anaconda3/lib/python3.7/site-packages/nltk/stem/porter.py\", line 668, in stem\n",
      "    stem = self._step1b(stem)\n",
      "  File \"/home/anmolagarwal/anaconda3/lib/python3.7/site-packages/nltk/stem/porter.py\", line 358, in _step1b\n",
      "    intermediate_stem = self._replace_suffix(word, suffix, '')\n",
      "  File \"/home/anmolagarwal/anaconda3/lib/python3.7/site-packages/nltk/stem/porter.py\", line 247, in _replace_suffix\n",
      "    return word[: -len(suffix)] + replacement\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anmolagarwal/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anmolagarwal/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1151, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/anmolagarwal/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/anmolagarwal/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/anmolagarwal/anaconda3/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/anmolagarwal/anaconda3/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/anmolagarwal/anaconda3/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/anmolagarwal/anaconda3/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "lb_file_idx=0\n",
    "ub_file_idx=102\n",
    "\n",
    "potential_dups_yet=0\n",
    "\n",
    "for file_id in range(lb_file_idx, ub_file_idx+1):\n",
    "    print(\"Starting \", file_id)\n",
    "    with open(PREFIX_FILE_PATH+f\"/post_{file_id}.json\",'r') as fd:\n",
    "        df=json.load(fd)\n",
    "    new_df=dict()\n",
    "    for curr_key, curr_val in df.items():\n",
    "        if curr_val[\"PostTypeId\"]!=\"1\":\n",
    "            continue\n",
    "        new_val=deepcopy(curr_val)\n",
    "        new_val['cleaned_body']=rem_code(new_val['Body'])\n",
    "        new_val['cleaned_body']=clean_tags_prevent_content(new_val['cleaned_body'])\n",
    "        new_val['cleaned_body']=clean_newline_stuff(new_val['cleaned_body'])\n",
    "        \n",
    "        new_val['cleaned_title']=rem_dup(new_val['Title'])\n",
    "        \n",
    "        new_val['body_vec']=preprocess_object.parse_string(new_val['cleaned_body'])\n",
    "        new_val['title_vec']=preprocess_object.parse_string(new_val['cleaned_title'])\n",
    "        \n",
    "        try:\n",
    "            new_val['dups_list']=dups_dict[curr_key]\n",
    "            potential_dups_yet+=1\n",
    "        except:\n",
    "            new_val['dups_list']=[]\n",
    "        try:\n",
    "            new_val['tags_list']=fetch_tags_list(new_val['Tags'])\n",
    "        except:\n",
    "            new_val['tags_list']=[]\n",
    "            print(curr_val)\n",
    "            #break\n",
    "        new_df[curr_key]=deepcopy(new_val)\n",
    "    print(\"Finished \", file_id)\n",
    "    print(\"dups potential \", potential_dups_yet)\n",
    "    print(\"-----------\")\n",
    "    with open(NEW_PREFIX_FILE_PATH+f\"/post_{file_id}.json\",'w') as fd:\n",
    "        json.dump(new_df, fd, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['784929']\n",
      "['918886']\n",
      "['39476']\n",
      "['628301']\n",
      "['405953']\n",
      "['970477']\n",
      "['7421324']\n",
      "['232535']\n",
      "['65035']\n",
      "['4760215']\n",
      "['2077807']\n",
      "['1043402']\n",
      "['1407743']\n",
      "['872272']\n",
      "['750486']\n",
      "['694929']\n",
      "['630803']\n",
      "['1082162']\n",
      "['1417740']\n",
      "['473620']\n",
      "['1140140']\n",
      "['498970']\n",
      "['752455']\n",
      "['1391718']\n",
      "['5164930']\n",
      "['588004']\n",
      "['1420931']\n",
      "['163542']\n",
      "['4448329']\n",
      "['10517268']\n",
      "['359424']\n",
      "['1144051']\n",
      "['2268276']\n",
      "['1036665']\n",
      "['1710875']\n",
      "['1397593']\n",
      "['5391795']\n",
      "['49906']\n",
      "['391523']\n",
      "['48872']\n",
      "['2375345']\n",
      "['122670']\n",
      "['22444']\n",
      "['1031554']\n",
      "['3366529']\n",
      "['3422673']\n",
      "['1310166']\n",
      "['80875']\n",
      "['5727']\n",
      "['246666']\n",
      "['8002445']\n",
      "['6739871']\n",
      "['436374']\n",
      "['519751']\n",
      "['120951']\n",
      "['4535328']\n",
      "['865668']\n",
      "['3003145']\n",
      "['3276156']\n",
      "['1097367']\n",
      "['1402505']\n",
      "['102631']\n",
      "['10829870']\n",
      "['1209181']\n",
      "['1072069']\n",
      "['994143']\n",
      "['310870']\n",
      "['659013']\n",
      "['305179']\n",
      "['10419577', '1813881']\n",
      "['1251999']\n",
      "['1006537']\n",
      "['1812244']\n",
      "['214615']\n",
      "['929684', '24481', '126207']\n",
      "['595957']\n",
      "['6140051']\n",
      "['898669']\n",
      "['8460037']\n",
      "['68372']\n",
      "['951021']\n",
      "['3349753']\n",
      "['2006626']\n",
      "['2037203']\n",
      "['4076601']\n",
      "['1479361']\n",
      "['5762491']\n",
      "['2987195']\n",
      "['1601151']\n",
      "['732684', '1242705']\n",
      "['32664']\n",
      "['750486']\n",
      "['271561']\n",
      "['258897']\n",
      "['381542']\n",
      "['1003177']\n",
      "['719877']\n",
      "['48249']\n",
      "['857420']\n"
     ]
    }
   ],
   "source": [
    "for key in new_df:\n",
    "    if new_df[key]['dups_list']!=[]:\n",
    "        print(new_df[key]['dups_list'])\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
