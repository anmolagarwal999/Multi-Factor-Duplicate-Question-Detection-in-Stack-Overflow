{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from preprocessing import Preprocess as preprocess_class\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_object=preprocess_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX_INPUT_FILE_PATH=\"data_files/pp_nov_28/result/\"\n",
    "PREFIX_OUTPUT_FILE_PATH=\"data_files/cleaned_pp_nov_28/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_file_idx=0\n",
    "ub_file_idx=102\n",
    "\n",
    "potential_dups_yet=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To clean self-closing tags and both closing tags from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/a/11229866\n",
    "self_contained_ref_regex=r\"<[^>]*>\"\n",
    "self_contained_ref_pattern=re.compile(self_contained_ref_regex)\n",
    "\n",
    "def clean_tags_prevent_content(curr_text):\n",
    "    curr_text=self_contained_ref_pattern.sub(\" \",curr_text)\n",
    "    return curr_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_cases=[\"< Hello> There </Hello>\",\n",
    "            \"billy butcher\",\n",
    "              \"<Doncaster/>\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tc is  < Hello> There </Hello>\n",
      "Output  is    There  \n",
      "**********\n",
      "tc is  billy butcher\n",
      "Output  is  billy butcher\n",
      "**********\n",
      "tc is  <Doncaster/>\n",
      "Output  is   \n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "for curr_tc in tc_cases:\n",
    "    print(\"tc is \",curr_tc)\n",
    "    print(\"Output  is \",clean_tags_prevent_content(curr_tc) )\n",
    "    print(\"**********\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning newlines and tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_newline_stuff(curr_text):\n",
    "    curr_text=curr_text.replace(\"\\n\",\"\")\n",
    "    curr_text=curr_text.replace(\"\\t\",\"\")\n",
    "    return curr_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching question tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_tags_list(tag_text):\n",
    "    arr=tag_text.split(\">\")\n",
    "    arr=list(filter(lambda x:x!=\"\",arr))\n",
    "    arr=[x[1:] for x in arr]\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_cases=['<performance><unix><awk><aix>',\n",
    "            '<c#><exception><error-handling>',\n",
    "              \"<Doncaster>\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tc is  <performance><unix><awk><aix>\n",
      "Output  is  ['performance', 'unix', 'awk', 'aix']\n",
      "**********\n",
      "tc is  <c#><exception><error-handling>\n",
      "Output  is  ['c#', 'exception', 'error-handling']\n",
      "**********\n",
      "tc is  <Doncaster>\n",
      "Output  is  ['Doncaster']\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "for curr_tc in tc_cases:\n",
    "    print(\"tc is \",curr_tc)\n",
    "    print(\"Output  is \",fetch_tags_list(curr_tc) )\n",
    "    print(\"**********\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing all variations of the word \"DUPLICATE\" from title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_regex = re.compile(\"duplicate\", re.IGNORECASE)\n",
    "def rem_dup(text):\n",
    "    return dup_regex.sub(\" \", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing \"code\"/blockquote tag and the content inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_paired_refs_regex=r\"<code(.*?)<\\/code>\"\n",
    "code_paired_refs_pattern=re.compile(code_paired_refs_regex)\n",
    "def rem_code(text):\n",
    "    return code_paired_refs_pattern.sub(\" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "blockquote_paired_refs_regex=r\"<blockquote(.*?)<\\/blockquote>\"\n",
    "blockquote_paired_refs_pattern=re.compile(blockquote_paired_refs_regex)\n",
    "def rem_blockquote(text):\n",
    "    return blockquote_paired_refs_pattern.sub(\" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_question_satisfying_a_condition(fn):\n",
    "    ans=None\n",
    "    for file_id in range(lb_file_idx, ub_file_idx+1):\n",
    "        if ans!=None:\n",
    "            break\n",
    "        print(\"Starting file with id: \", file_id)\n",
    "        with open(PREFIX_FILE_PATH+f\"/post_{file_id}.json\",'r') as fd:\n",
    "            df=json.load(fd)\n",
    "        new_df=dict()\n",
    "        for curr_key, curr_val in df.items():\n",
    "            # we do not want to process answers\n",
    "            if curr_val[\"PostTypeId\"]!=\"1\":\n",
    "                continue\n",
    "            if fn(curr_val):\n",
    "                ans=deepcopy(curr_val)\n",
    "                print(\"Found\")\n",
    "                break\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_tag_in_body(curr_obj):\n",
    "    all_matches=code_paired_refs_pattern.findall(curr_obj['Body'])\n",
    "    return len(all_matches)>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blockquote_tag_in_body(curr_obj):\n",
    "    all_matches=blockquote_paired_refs_pattern.findall(curr_obj['Body'])\n",
    "    return len(all_matches)>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting file with id:  0\n",
      "Found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AcceptedAnswerId': '7',\n",
       " 'AnswerCount': '13',\n",
       " 'Body': \"<p>I'm new to C# and I want to use a track-bar to change a form's opacity.</p>\\n\\n<p>This is my code:</p>\\n\\n<pre><code>decimal trans = trackBar1.Value / 5000;\\nthis.Opacity = trans;\\n</code></pre>\\n\\n<p>When I try to build it, I get this error:</p>\\n\\n<blockquote>\\n  <p>Cannot implicitly convert type 'decimal' to 'double'</p>\\n</blockquote>\\n\\n<p>I tried making <code>trans</code> a double, but then the control doesn't work. This code worked fine for me in VB.NET. </p>\\n\\n<p>What do I need to do differently?</p>\\n\",\n",
       " 'CommentCount': '19',\n",
       " 'CreationDate': '2008-07-31T21:42:52.667',\n",
       " 'FavoriteCount': '14',\n",
       " 'Id': '4',\n",
       " 'LastActivityDate': '2012-07-24T16:45:40.937',\n",
       " 'LastEditDate': '2012-05-04T08:55:46.677',\n",
       " 'LastEditorDisplayName': 'Rich B',\n",
       " 'LastEditorUserId': '1039608',\n",
       " 'OwnerUserId': '8',\n",
       " 'PostTypeId': '1',\n",
       " 'Score': '139',\n",
       " 'Tags': '<c#><winforms><forms><type-conversion><opacity>',\n",
       " 'Title': \"When setting a form's opacity should I use a decimal or double?\",\n",
       " 'ViewCount': '9959'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_question_satisfying_a_condition(code_tag_in_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_question_satisfying_a_condition(blockquote_tag_in_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_question_id_object(q_id):\n",
    "    ans=None\n",
    "    assert(type(q_id)==str)\n",
    "    for file_id in range(lb_file_idx, ub_file_idx+1):\n",
    "        if ans!=None:\n",
    "            break\n",
    "        print(\"Starting file with id: \", file_id)\n",
    "        with open(PREFIX_FILE_PATH+f\"/post_{file_id}.json\",'r') as fd:\n",
    "            df=json.load(fd)\n",
    "        new_df=dict()\n",
    "        for curr_key, curr_val in df.items():\n",
    "            # we do not want to process answers\n",
    "            if curr_val[\"PostTypeId\"]!=\"1\":\n",
    "                continue\n",
    "            if curr_val['Id']==q_id:\n",
    "                ans=deepcopy(curr_val)\n",
    "                print(\"Found\")\n",
    "                break\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting file with id:  0\n",
      "Starting file with id:  1\n",
      "Starting file with id:  2\n",
      "Starting file with id:  3\n",
      "Starting file with id:  4\n",
      "Starting file with id:  5\n",
      "Found\n",
      "Starting file with id:  6\n",
      "Starting file with id:  7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-a5688e2632f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfetch_question_id_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"777711\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-88-697c5e040288>\u001b[0m in \u001b[0;36mfetch_question_id_object\u001b[0;34m(q_id)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting file with id: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPREFIX_FILE_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34mf\"/post_{file_id}.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mnew_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcurr_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fetch_question_id_object(\"777711\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just investigate the PostTypeIDs present (DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_post_types():\n",
    "    for curr_id, curr_val in df.items():\n",
    "        if curr_val['PostTypeId']!='1':\n",
    "            print(curr_val['PostTypeId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, first find a duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reg=re.compile(r\"\\[\\s*duplicate\\s*\\]\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dup_title_string=[\"[  duplicaTe]\"\n",
    "                            ,\"[   duplicaTE       ]\"\n",
    "                             ,\"[duplicaTE]\"\n",
    "                          ,\"abcdef [duplicaTE] dupl sis os\"        ,\n",
    "                          \"skjsis9\"\n",
    "                         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  duplicaTe]\n",
      "['[  duplicaTe]']\n",
      "##########\n",
      "[   duplicaTE       ]\n",
      "['[   duplicaTE       ]']\n",
      "##########\n",
      "[duplicaTE]\n",
      "['[duplicaTE]']\n",
      "##########\n",
      "abcdef [duplicaTE] dupl sis os\n",
      "['[duplicaTE]']\n",
      "##########\n",
      "skjsis9\n",
      "[]\n",
      "##########\n"
     ]
    }
   ],
   "source": [
    "for curr_tc in testing_dup_title_string:\n",
    "    matches=test_reg.findall(curr_tc)\n",
    "    print(curr_tc)\n",
    "    print(matches)\n",
    "    print(\"##########\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all questions with [duplicate] in their title among all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dups_in_title():\n",
    "    lb_file_idx=0\n",
    "    ub_file_idx=102\n",
    "\n",
    "    potential_dups_yet=0\n",
    "\n",
    "    for file_id in range(lb_file_idx, ub_file_idx+1):\n",
    "        print(\"Starting file with id: \", file_id)\n",
    "        with open(PREFIX_FILE_PATH+f\"/post_{file_id}.json\",'r') as fd:\n",
    "            df=json.load(fd)\n",
    "        new_df=dict()\n",
    "        for curr_key, curr_val in df.items():\n",
    "            # we do not want to process answers\n",
    "            if curr_val[\"PostTypeId\"]!=\"1\":\n",
    "                continue\n",
    "            matches=test_reg.findall(curr_val['Title'])\n",
    "            if len(matches)>0:\n",
    "                print(curr_key)\n",
    "                print(curr_val)\n",
    "                print(\"$$$$$$$$$$$$$$$$$$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting file with id:  0\n",
      "Starting file with id:  1\n",
      "Starting file with id:  2\n",
      "Starting file with id:  3\n",
      "Starting file with id:  4\n",
      "Starting file with id:  5\n",
      "777711\n",
      "{'AcceptedAnswerId': '777721', 'AnswerCount': '4', 'Body': '<blockquote>\\n  <p>Duplicate:\\n  <a href=\"http://stackoverflow.com/questions/163434/are-nulls-in-a-relational-database-okay\">http://stackoverflow.com/questions/163434/are-nulls-in-a-relational-database-okay</a></p>\\n</blockquote>\\n\\n<p>I dodged a heated debate concerning nulls in the database today.\\nMy opinion is that null is an excellent indicator of unspecified values. Every one else in the team, that has an opinion, thinks zero and empty strings are the way to go.</p>\\n\\n<p>Are they lazy or am I to strict?</p>\\n', 'CommentCount': '1', 'CreationDate': '2009-04-22T15:07:54.137', 'Id': '777711', 'LastActivityDate': '2009-04-22T15:11:00.667', 'LastEditDate': '2009-04-22T15:10:22.710', 'LastEditorDisplayName': '', 'LastEditorUserId': '44389', 'OwnerUserId': '21761', 'PostTypeId': '1', 'Score': '0', 'Tags': '<asp.net><sql><database><null>', 'Title': 'Is null harmful? [Duplicate]', 'ViewCount': '175'}\n",
      "$$$$$$$$$$$$$$$$$$\n",
      "783926\n",
      "{'AcceptedAnswerId': '783939', 'AnswerCount': '7', 'Body': '<blockquote>\\n  <h2>Duplicate</h2>\\n  \\n  <p><a href=\"http://stackoverflow.com/questions/403539/what-are-extension-methods\">What are Extension Methods?</a><br />\\n  <a href=\"http://stackoverflow.com/questions/403619/usage-of-extension-methods\">Usage of Extension Methods</a><br />\\n  <a href=\"http://stackoverflow.com/questions/487904/what-advantages-of-extension-methods-have-you-found\">What Advantages of Extension Methods have you found?</a>  </p>\\n</blockquote>\\n\\n<p>So I run into the term \"extension-methods\" frequently, when reading about .Net and intellisensing (!) around...</p>\\n\\n<p>What are extension-methods -- and what sets them apart from other methods?</p>\\n', 'CommentCount': '5', 'CreationDate': '2009-04-23T23:10:37.093', 'FavoriteCount': '1', 'Id': '783926', 'LastActivityDate': '2009-04-28T07:01:13.167', 'LastEditDate': '2009-04-23T23:17:21.163', 'LastEditorDisplayName': '', 'LastEditorUserId': '44389', 'OwnerUserId': '77884', 'PostTypeId': '1', 'Score': '0', 'Tags': '<.net><extension-methods>', 'Title': 'What are extension-methods -- and what makes them different from other methods? [Duplicate]', 'ViewCount': '153'}\n",
      "$$$$$$$$$$$$$$$$$$\n",
      "Starting file with id:  6\n",
      "Starting file with id:  7\n",
      "Starting file with id:  8\n",
      "Starting file with id:  9\n",
      "Starting file with id:  10\n",
      "Starting file with id:  11\n",
      "Starting file with id:  12\n",
      "Starting file with id:  13\n",
      "Starting file with id:  14\n",
      "Starting file with id:  15\n",
      "Starting file with id:  16\n",
      "Starting file with id:  17\n",
      "Starting file with id:  18\n",
      "Starting file with id:  19\n",
      "Starting file with id:  20\n",
      "Starting file with id:  21\n",
      "Starting file with id:  22\n",
      "Starting file with id:  23\n",
      "Starting file with id:  24\n",
      "Starting file with id:  25\n",
      "Starting file with id:  26\n",
      "Starting file with id:  27\n",
      "Starting file with id:  28\n",
      "Starting file with id:  29\n",
      "Starting file with id:  30\n",
      "Starting file with id:  31\n",
      "Starting file with id:  32\n",
      "Starting file with id:  33\n",
      "Starting file with id:  34\n",
      "Starting file with id:  35\n",
      "Starting file with id:  36\n",
      "Starting file with id:  37\n",
      "Starting file with id:  38\n",
      "Starting file with id:  39\n",
      "Starting file with id:  40\n",
      "Starting file with id:  41\n",
      "Starting file with id:  42\n",
      "Starting file with id:  43\n",
      "Starting file with id:  44\n",
      "Starting file with id:  45\n",
      "Starting file with id:  46\n",
      "Starting file with id:  47\n",
      "Starting file with id:  48\n",
      "Starting file with id:  49\n",
      "Starting file with id:  50\n",
      "Starting file with id:  51\n",
      "Starting file with id:  52\n",
      "Starting file with id:  53\n",
      "Starting file with id:  54\n",
      "Starting file with id:  55\n",
      "Starting file with id:  56\n",
      "Starting file with id:  57\n",
      "Starting file with id:  58\n",
      "Starting file with id:  59\n",
      "Starting file with id:  60\n",
      "Starting file with id:  61\n",
      "Starting file with id:  62\n",
      "Starting file with id:  63\n",
      "Starting file with id:  64\n",
      "Starting file with id:  65\n",
      "Starting file with id:  66\n",
      "Starting file with id:  67\n",
      "Starting file with id:  68\n",
      "Starting file with id:  69\n",
      "Starting file with id:  70\n",
      "Starting file with id:  71\n",
      "Starting file with id:  72\n",
      "Starting file with id:  73\n",
      "Starting file with id:  74\n",
      "Starting file with id:  75\n",
      "Starting file with id:  76\n",
      "Starting file with id:  77\n",
      "Starting file with id:  78\n",
      "Starting file with id:  79\n",
      "Starting file with id:  80\n",
      "Starting file with id:  81\n",
      "Starting file with id:  82\n",
      "Starting file with id:  83\n",
      "Starting file with id:  84\n",
      "Starting file with id:  85\n",
      "Starting file with id:  86\n",
      "Starting file with id:  87\n",
      "Starting file with id:  88\n",
      "Starting file with id:  89\n",
      "Starting file with id:  90\n",
      "Starting file with id:  91\n",
      "Starting file with id:  92\n",
      "Starting file with id:  93\n",
      "Starting file with id:  94\n",
      "Starting file with id:  95\n",
      "Starting file with id:  96\n",
      "Starting file with id:  97\n",
      "Starting file with id:  98\n",
      "Starting file with id:  99\n",
      "Starting file with id:  100\n",
      "Starting file with id:  101\n",
      "Starting file with id:  102\n"
     ]
    }
   ],
   "source": [
    "find_dups_in_title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for file_id in range(lb_file_idx, ub_file_idx+1):\n",
    "    print(\"Starting file with id: \", file_id)\n",
    "    with open(PREFIX_FILE_PATH+f\"/post_{file_id}.json\",'r') as fd:\n",
    "        df=json.load(fd)\n",
    "    new_df=dict()\n",
    "    for curr_key, curr_val in df.items():\n",
    "        # we do not want to process answers\n",
    "        if curr_val[\"PostTypeId\"]!=\"1\":\n",
    "            continue\n",
    "        new_val=deepcopy(curr_val)\n",
    "        new_val['cleaned_body']=rem_code(new_val['Body'])\n",
    "        new_val['cleaned_body']=clean_tags_prevent_content(new_val['cleaned_body'])\n",
    "        new_val['cleaned_body']=clean_newline_stuff(new_val['cleaned_body'])\n",
    "        \n",
    "        new_val['cleaned_title']=rem_dup(new_val['Title'])\n",
    "        \n",
    "        new_val['body_vec']=preprocess_object.parse_string(new_val['cleaned_body'])\n",
    "        new_val['title_vec']=preprocess_object.parse_string(new_val['cleaned_title'])\n",
    "        \n",
    "        #try:\n",
    "            #new_val['dups_list']=dups_dict[curr_key]\n",
    "            #potential_dups_yet+=1\n",
    "        #except:\n",
    "            #new_val['dups_list']=[]\n",
    "        try:\n",
    "            new_val['tags_list']=fetch_tags_list(new_val['Tags'])\n",
    "        except:\n",
    "            new_val['tags_list']=[]\n",
    "            print(curr_val)\n",
    "            #break\n",
    "        new_df[curr_key]=deepcopy(new_val)\n",
    "    print(\"Finished \", file_id)\n",
    "    print(\"dups potential \", potential_dups_yet)\n",
    "    print(\"-----------\")\n",
    "    with open(NEW_PREFIX_FILE_PATH+f\"/post_{file_id}.json\",'w') as fd:\n",
    "        json.dump(new_df, fd, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in new_df:\n",
    "    if new_df[key]['dups_list']!=[]:\n",
    "        print(new_df[key]['dups_list'])\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
